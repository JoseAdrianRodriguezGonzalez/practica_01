El aprendizaje supervisado es una técnica de machine learning que utiliza conjuntos de datos de entrada y salida etiquetados por humanos para entrenar modelos de inteligencia artificial. El modelo entrenado aprende las relaciones subyacentes entre las entradas y las salidas, lo que le permite predecir las salidas correctas basadas en nuevas entradas no etiquetadas del mundo real. Los datos etiquetados consisten en puntos de datos de ejemplo junto con las salidas o respuestas correctas. A medida que los datos de entrada se introducen en el algoritmo de machine learning, este reajusta sus ponderaciones hasta que el modelo se ajuste correctamente. Los datos de entrenamiento etiquetados enseñan explícitamente al modelo a identificar las relaciones entre las características y las etiquetas de datos.  El machine learning supervisado ayuda a las organizaciones a resolver diversos problemas del mundo real a escala, como clasificar el spam o predecir los precios de las acciones. Se puede utilizar para crear modelos de machine learning de alta precisión. Cómo funciona el aprendizaje supervisado El aprendizaje supervisado utiliza un conjunto de datos de entrenamiento etiquetado para comprender las relaciones entre los datos de entrada y salida. Los científicos de datos crean manualmente conjuntos de datos de entrenamiento que contienen datos de entrada junto con las etiquetas correspondientes. El aprendizaje supervisado entrena al modelo para aplicar los resultados correctos a los nuevos datos de entrada en casos de uso del mundo real. Durante el entrenamiento, el algoritmo del modelo procesa grandes conjuntos de datos para explorar posibles correlaciones entre entradas y salidas. A continuación, se evalúa el rendimiento del modelo con datos de prueba para averiguar si se entrenó correctamente. La validación cruzada es el proceso de probar un modelo utilizando una parte diferente del conjunto de datos. 
El análisis de regresión lineal es una técnica de analisis de datos que se utiliza para predecir el valor de una variable en función del valor de otra variable. La variable que desea predecir se llama dependiente. La variable que está utilizando para predecir el valor de la otra variable se llama independiente. Esta forma de análisis estima los coeficientes de la ecuación lineal, involucrando una o más variables independientes que predicen mejor el valor de la variable dependiente. La regresión lineal se ajusta a una línea recta o superficie que minimiza las discrepancias entre los valores de salida previstos y reales. Existen calculadoras de regresión lineal simples que utilizan un método de "mínimos cuadrados" para descubrir la línea de mejor ajuste para un conjunto de datos pareados. Luego, estime el valor de X (variable dependiente) a partir de Y (variable independiente).
¿Qué es la regresión logística? La regresión logística estima la probabilidad de que ocurra un evento, como votar o no votar, en función de un conjunto de datos dado de variables independientes. Este tipo de modelo estadístico (también conocido como modelo logit) se utiliza a menudo para la clasificación y el análisis predictivo. Dado que el resultado es una probabilidad, la variable dependiente está acotada entre 0 y 1. En la regresión logística, se aplica una transformación logit sobre las probabilidades, es decir, la probabilidad de éxito dividida por la probabilidad de fracaso. También se conoce comúnmente como logaritmo de probabilidades, o logaritmo natural de probabilidades, y esta función logística se representa mediante las siguientes fórmulas: Logit(pi) = 1/(1+ exp(-pi)) ln(pi/(1-pi)) = Beta_0 + Beta_1*X_1 + … + B_k*K_k En esta ecuación de regresión logística, logit(pi) es la variable dependiente o de respuesta y x es la variable independiente. El parámetro beta, o coeficiente, en este modelo se estima comúnmente mediante estimación de máxima verosimilitud (EMV). Este método prueba diferentes valores de beta a través de múltiples iteraciones para optimizar el mejor ajuste de las probabilidades logarítmicas. Todas estas iteraciones producen la función de verosimilitud logarítmica y la regresión logística busca maximizar esta función para encontrar la mejor estimación de parámetros. Una vez que se encuentra el coeficiente óptimo (o los coeficientes, si hay más de una variable independiente), las probabilidades condicionales para cada observación se pueden calcular, registrar y sumar para obtener una probabilidad prevista. Para la clasificación binaria, una probabilidad menor que 5 predecirá 0, mientras que una probabilidad mayor que 0 predecirá 1. Una vez calculado el modelo, la mejor práctica consiste en evaluar en qué medida el modelo predice la variable dependiente, lo que se denomina bondad de ajuste.
¿Qué es un árbol de decisión? Un árbol de decisión es un algoritmo de aprendizaje supervisado no paramétrico, que se utiliza tanto para tareas de clasificación como de regresión. Tiene una estructura jerárquica de árbol, que consta de un nodo raíz, ramas, nodos internos y nodos hoja. Como puede ver en el siguiente diagrama, un árbol de decisión comienza con un nodo raíz, que no tiene ninguna rama entrante. Las ramas salientes del nodo raíz luego alimentan los nodos internos, también conocidos como nodos de decisión. En función de las características disponibles, ambos tipos de nodos realizan evaluaciones para formar subconjuntos homogéneos, que se denotan mediante nodos hoja o nodos terminales. Los nodos hoja representan todos los resultados posibles dentro del conjunto de datos.
¿Qué es el algoritmo KNN? El algoritmo KNN (k-nearest neighbors) es un clasificador de aprendizaje supervisado no paramétrico, que emplea la proximidad para realizar clasificaciones o predicciones sobre la agrupación de un punto de datos individual. Es uno de los clasificadores de clasificación y regresión más populares y sencillos que se emplean actualmente en machine learning. Si bien el algoritmo KNN se puede usar para problemas de regresión o clasificación, generalmente se usa como un algoritmo de clasificación, partiendo del supuesto de que se pueden encontrar puntos similares cerca uno del otro. En los problemas de clasificación, la etiqueta de clase se asigna por mayoría de votos, es decir, se emplea la etiqueta que se representa con más frecuencia en torno a un punto de datos determinado. Aunque técnicamente se considera "votación por pluralidad", el término "votación por mayoría" se emplea más comúnmente en las publicaciones al respecto. La distinción entre estas terminologías es que la "votación por mayoría" requiere técnicamente una mayoría superior al 50%, lo que funciona sobre todo cuando sólo hay dos categorías. Cuando hay varias clases, por ejemplo, cuatro categorías, usted no requiere necesariamente el 50% de los votos para llegar a una conclusión sobre una clase, ya que podría asignar una etiqueta de clase con un voto superior al 25%
¿Qué son las SVM? Una máquina de vectores de soporte (SVM) es un algoritmo de aprendizaje automático supervisado que clasifica los datos al encontrar una línea o hiperplano óptimo que maximice la distancia entre cada clase en un espacio N-dimensional. Las SVM fueron desarrolladas en la década de 1990 por Vladimir N. Vapnik y sus colegas, y publicaron este trabajo en un artículo titulado "Support Vector Method for Function Approximation, Regression Estimation, and Signal Processing"1 en 1995. Las SVM se emplean comúnmente en problemas de clasificación. Distinguen entre dos clases encontrando el hiperplano óptimo que maximiza el margen entre los puntos de datos más cercanos de clases opuestas. El número de características en los datos de entrada determina si el hiperplano es una línea en un espacio bidimensional o un plano en un espacio n-dimensional. Dado que se pueden encontrar múltiples hiperplanos para diferenciar clases, la maximización del margen entre puntos permite al algoritmo encontrar la mejor frontera de decisión entre clases. Esto, a su vez, le permite generalizar bien nuevos datos y hacer predicciones de clasificación precisas. Las líneas adyacentes al hiperplano óptimo se conocen como vectores de soporte, ya que estos vectores atraviesan los puntos de datos que determinan el margen máximo. El algoritmo SVM se emplea ampliamente en el aprendizaje automático ya que puede manejar tareas de clasificación tanto lineales como no lineales. Sin embargo, cuando los datos no son separables linealmente, las funciones del kernel se utilizan para transformar el espacio multidimensional de los datos y permitir la separación lineal.
¿Qué es el bosque aleatorio? El bosque aleatorio es un algoritmo de aprendizaje automático de uso común, registrado por Leo Breiman y Adele Cutler, que combina el resultado de múltiples árboles de decisión para llegar a un resultado único. Su facilidad de uso y flexibilidad han impulsado su adopción, ya que maneja problemas de clasificación y regresión. Árboles de decisión Dado que el modelo de bosque aleatorio se compone de varios árboles de decisión, sería útil empezar describiendo brevemente el algoritmo del árbol de decisión. Los árboles de decisión comienzan con una pregunta básica, como "¿Debería navegar?" A partir de ahí, puede hacer una serie de preguntas para determinar una respuesta, como "¿El oleaje es prolongado?" o "¿Hay viento en alta mar?". Estas preguntas constituyen los nodos de decisión en el árbol, que funcionan como un medio para dividir los datos. Cada pregunta ayuda a un individuo a llegar a una decisión final, que sería señalada por el nodo hoja. Las observaciones que cumplan con los criterios seguirán el ramal "Sí" y las que no, seguirán la ruta alternativa. Los árboles de decisión buscan encontrar la mejor división para los subconjuntos de datos y, por lo general, se entrenan a través del algoritmo del árbol de clasificación y regresión (CART). Las métricas, como la impureza de Gini, la ganancia de información o el error cuadrático medio (MSE), pueden utilizarse para evaluar la calidad de la división. Este árbol de decisión es un ejemplo de un problema de clasificación, donde las etiquetas de clase son "navegar" y "no navegar". Si bien los árboles de decisión son algoritmos comunes de aprendizaje supervisado, pueden ser proclives a presentar problemas, como sesgos y sobreajuste. Sin embargo, cuando varios árboles de decisión forman un conjunto en el algoritmo de bosque aleatorio, predicen resultados más precisos, en especial cuando los árboles de decisión individuales no están correlacionados entre sí.
Funcionamiento de las redes CNN Las redes neuronales convolucionales pueden tener decenas o cientos de capas, y cada una de ellas aprende a detectar diferentes características de una imagen. Se aplican filtros a las imágenes de entrenamiento con distintas resoluciones, y la salida resultante de convolucionar cada imagen se emplea como entrada para la siguiente capa. Los filtros pueden comenzar como características muy simples, tales como brillo y bordes, e ir creciendo en complejidad hasta convertirse en características que definen el objeto de forma singular. Aprendizaje de características, capas y clasificación Una CNN consta de una capa de entrada, una capa de salida y varias capas ocultas entre ambas. Estas capas realizan operaciones que modifican los datos, con el propósito de comprender sus características particulares. Las 3 capas más comunes son: convolución, activación o ReLU, y agrupación. Convolución: Aplica un conjunto de filtros convolucionales a las imágenes de entrada; cada filtro activa diferentes características de las imágenes. Unidad lineal rectificada (ReLU): Mantiene los valores positivos y establece los valores negativos en cero, lo que permite un entrenamiento más rápido y eficaz. También se lo conoce como activación, ya que solo las características activadas prosiguen a la siguiente capa. Agrupación: Simplifica la salida mediante reducción no lineal de la tasa de muestreo, lo que disminuye el número de parámetros que la red debe aprender.
¿Qué son los clasificadores Naive Bayes? El clasificador Naive Bayes es un algoritmo de machine learning supervisado que se utiliza para tareas de clasificación como la clasificación de textos. Utiliza principios de probabilidad para realizar tareas de clasificación. Naïve Bayes forma parte de una familia de algoritmos de aprendizaje generativo, lo que significa que busca modelar la distribución de las entradas de una clase o categoría determinada. A diferencia de los clasificadores discriminativos, como la regresión logística, no aprende qué características son las más importantes para diferenciar entre clases. Breve repaso a la estadística bayesiana El Naïve Bayes también se conoce como clasificador probabilístico, ya que se basa en el teorema de Bayes. Sería difícil explicar este algoritmo sin explicar los fundamentos de la estadística bayesiana. Este teorema, también conocido como regla de Bayes, nos permite "invertir" las probabilidades condicionales. Como recordatorio, las probabilidades condicionales representan la probabilidad de que un evento se produzca dado que se ha producido algún otro evento, lo que se representa con la siguiente fórmula:
¿Qué es ANN en IA? Una red neuronal artificial o artificial neural network (ANN) es un modelo matemático que imita el procesamiento de información de una red neuronal biológica. Está compuesta por nodos interconectados, que simulan las neuronas biológicas, y capas, que representan las conexiones entre estas neuronas. Cada nodo tiene un peso y un valor de umbral, y cuando la salida de un nodo supera el umbral, se activa y envía su información a la siguiente capa de la red. Las redes neuronales artificiales son capaces de aprender y mejorar su precisión a medida que se les suministra más información de entrenamiento. Utilizan algoritmos de aprendizaje automático para ajustar los pesos y los umbrales de los nodos, de manera que la red pueda realizar tareas específicas de manera más eficiente y precisa. El aprendizaje automático como base de las ANN El aprendizaje automático es un subconjunto de la inteligencia artificial que permite la optimización de los algoritmos. A través del aprendizaje automático, las redes neuronales artificiales pueden hacer predicciones y minimizar los errores derivados de la simple suposición. Por ejemplo, empresas como Amazon utilizan el aprendizaje automático para recomendar productos a sus clientes en función de sus preferencias y comportamientos anteriores. El aprendizaje automático se divide en varias categorías, como el aprendizaje supervisado, el aprendizaje no supervisado, el aprendizaje por refuerzo y el aprendizaje en línea. Cada una de estas categorías tiene sus propias características y aplicaciones específicas. 
¿Qué es una red neuronal recurrente? Una red neuronal recurrente o RNN es una red neuronal profunda que se entrena con datos secuenciales o de series temporales para crear un modelo de machine learning (ML) que pueda hacer predicciones o conclusiones secuenciales basándose en entradas secuenciales. Una RNN podría utilizarse para predecir los niveles diarios de inundación basándose en los datos diarios anteriores sobre inundaciones, mareas y meteorología. Pero las RNN también se pueden utilizar para resolver problemas ordinales o temporales, como la traducción de idiomas, el procesamiento del lenguaje natural (PLN), el análisis de sentimientos, el reconocimiento de voz y el subtitulado de imágenes. Cómo funcionan las RNN Al igual que las redes neuronales tradicionales, como las redes neuronales prealimentadas y las redes neuronales convolucionales (CNN), las redes neuronales recurrentes utilizan los datos de entrenamiento para aprender. Se distinguen por su "memoria", puesto que toman la información de las entradas anteriores para influir en la entrada y la salida actuales. Mientras que las redes tradicionales de deep learning asumen que las entradas y las salidas son independientes entre sí, las salidas de las redes neuronales recurrentes depende de los elementos anteriores dentro de la secuencia. Mientras que las redes neuronales profundas tradicionales asumen que las entradas y las salidas son independientes entre sí, la salida de las RNN depende de los elementos anteriores dentro de la secuencia.
La memoria larga a corto plazo, (en inglés, long short-term memory o LSTM) es una arquitectura de red neuronal recurrente (RNN) desarrollada para abordar el problema de desvanecimiento de gradiente,[1]​ que afecta la capacidad de las RNN tradicionales para aprender dependencias a largo plazo en secuencias de datos. A diferencia de otros modelos como los modelos ocultos de Markov, los LSTM pueden retener información durante miles de pasos temporales, lo que los convierte en una herramienta eficaz para procesar datos secuenciales.[2]​ El núcleo de un LSTM es la célula de memoria, que retiene información durante intervalos de tiempo arbitrarios. Esta célula está controlada por tres tipos de puertas: la puerta de entrada, la puerta de salida[3]​ y la puerta de olvido.[4]​ La puerta de entrada regula qué nueva información se almacenará en la célula de memoria, la puerta de salida decide qué información se usará para generar la salida en el paso actual, y la puerta de olvido determina qué información debe eliminarse.[2]​
¿Qué es la reducción de la dimensionalidad? Las técnicas de reducción de la dimensionalidad como PCA, LDA y t-SNE mejoran los modelos de machine learning. Conservan características esenciales de conjuntos de datos complejos al reducir el número de variables de previsión para una mayor generalización. La reducción de la dimensionalidad es un método para representar un conjunto de datos utilizando un menor número de características (es decir, dimensiones) sin perder las propiedades significativas de los datos originales.1 Esto equivale a eliminar características irrelevantes o redundantes, o simplemente datos ruidosos, para crear un modelo con un menor número de variables. La reducción de la dimensionalidad abarca una serie de métodos de selección de características y compresión de datos utilizados durante el preprocesamiento. Aunque los métodos de reducción de la dimensionalidad difieren en su funcionamiento, todos transforman espacios de alta dimensión en espacios de baja dimensión mediante la extracción o la combinación de variables.
El agrupamiento espacial basado en densidad de aplicaciones con ruido o Density-based spatial clustering of applications with noise (DBSCAN) es un algoritmo de agrupamiento de datos (data clustering) propuesto por Martin Ester, Hans-Peter Kriegel, Jörg Sander y Xiaowei Xu en 1996.[1]​Es un algoritmo de agrupamiento basado en densidad (density-based clustering) porque encuentra un número de grupos (clusters) comenzando por una estimación de la distribución de densidad de los nodos correspondientes. DBSCAN es uno de los algoritmos de agrupamiento más usados y citados en la literatura científica.[2]​ OPTICS puede verse como una generalización de DBSCAN para múltiples rangos, reemplazando el parámetro e por el radio máximo de búsqueda. En 2014, el algoritmo fue merecedor del premio a la prueba del tiempo (un reconocimiento dado a algoritmos que han recibido una sustancial atención en la teoría y la práctica) en la conferencia líder de la minería de datos, KDD.[3]​ El paper "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN" aparece en la lista de los 8 artículos más descargados en la revista ACM Transactions on Database Systems (TODS).
La agrupación de medias K es un algoritmo de aprendizaje no supervisado utilizado para la agrupación en clústeres de datos, que agrupa puntos de datos no etiquetados en grupos o clústeres. Es uno de los métodos de clustering más populares utilizados en el machine learning. A diferencia del aprendizaje supervisado, los datos de entrenamiento que utiliza este algoritmo no están etiquetados, lo que significa que los puntos de datos no tienen una estructura de clasificación definida. Aunque existen varios tipos de algoritmos de clustering, incluidos los exclusivos, los superpuestos, los jerárquicos y los probabilísticos, el algoritmo de clustering de medias k es un ejemplo de método de clustering exclusivo o "duro". Esta forma de agrupación estipula que un punto de datos puede existir en un solo clúster. Este tipo de análisis de clústeres se utiliza habitualmente en la ciencia de datos para la segmentación de mercados, la agrupación de documentos, la segmentación de imágenes y la compresión de imágenes. El algoritmo de medias k es un método ampliamente utilizado en el análisis de clústeres porque es eficiente, eficaz y sencillo. Las medias K son un algoritmo de clustering iterativo basado en centroides que divide un conjunto de datos en grupos similares en función de la distancia entre sus centroides. El centroide, o centro del clúster, es la media o la mediana de todos los puntos dentro del clúster, según las características de los datos.
BERT (Bidirectional Encoder Representations from Transformers) o Representación de Codificador Bidireccional de Transformadores es una técnica basada en redes neuronales para el pre-entrenamiento del procesamiento del lenguaje natural (PLN) desarrollada por Google.[1]​ BERT fue creado y publicado en 2018 por Jacob Devlin y sus compañeros en Google[2]​[3]​y su uso original fue para comprender mejor las búsquedas de las personas usuarias.[4]​ Lo que distingue a BERT es su capacidad para captar el contexto bidireccional en una oración. Es decir, que puede captar el significado de una palabra en relación con las palabras que la rodean. Esta mejora en la comprensión contextual ha llevado a un mayor rendimiento en diversas tareas de procesamiento del lenguaje natural. Lo cual ha sido aprovechado por Google para perfeccionar la comprensión de las búsquedas.[5]​ La implementación de BERT ha llevado a resultados de búsqueda más precisos y relevantes, ya que el motor de búsqueda puede entender mejor el contexto detrás de las consultas.
¿Qué es el PLN? El procesamiento del lenguaje natural (PLN) es un subcampo de la informática y la inteligencia artificial (IA) que utiliza el machine learning para permitir que los ordenadores entiendan y se comuniquen con el lenguaje humano. El PLN permite a los ordenadores y dispositivos digitales reconocer, comprender y generar texto y voz combinando la lingüística computacional (el modelado del lenguaje humano basado en reglas) junto con el modelado estadístico, el machine learning y el deep learning. La investigación en PLN ha ayudado a hacer posible la era de la IA generativa, desde las habilidades de comunicación de los modelos de lenguaje de gran tamaño (LLM) hasta la capacidad de los modelos de generación de imágenes para comprender las solicitudes. El PLN ya forma parte de la vida cotidiana de muchas personas, como los motores de búsqueda, los chatbots del servicio de atención al cliente con comandos hablados, los sistemas GPS controlados por voz y los asistentes digitales para responder preguntas en los smartphones, como Alexa de Amazon, Siri de Apple y Cortana de Microsoft.
ARIMA significa media móvil integrada autorregresiva y es una técnica para el análisis de series temporales y para pronosticar posibles valores futuros de una serie temporal. El modelado autorregresivo y el modelado de media móvil son dos enfoques diferentes para pronosticar datos de series temporales. ARIMA integra estos dos enfoques, de ahí el nombre. La Forecasting es una rama del machine learning que emplea el comportamiento pasado de una serie temporal para predecir uno o más valores futuros de esa serie temporal. Imagine que está comprando helado para abastecer una pequeña tienda. Si sabe que las ventas de helado aumentan constantemente a medida que el clima se calienta, probablemente debería predecir que el pedido de la próxima semana debería ser un poco mayor que el pedido de esta semana. El aumento dependerá de la diferencia entre las ventas de esta semana y las de la anterior No podemos pronosticar el futuro sin un pasado para compararlo, por lo que los datos de series temporales pasadas son muy importantes para ARIMA y para todos los métodos de forecasting y análisis de series temporales.
La exactitud es la proporción de todas las clasificaciones que fueron correctas, ya sean positivas o negativas. Se define matemáticamente de la siguiente manera: En el ejemplo de clasificación de spam, la precisión mide la fracción de todos los correos electrónicos clasificados correctamente. Un modelo perfecto no tendría ningún falso positivo ni ningún falso negativo y, por lo tanto, tendría una precisión de 1.0 o 100%. Debido a que incorpora los cuatro resultados de la matriz de confusión (VP, FP, TN y FN), dado un conjunto de datos equilibrado, con cantidades similares de ejemplos en ambas clases, la precisión puede servir como una medida de calidad del modelo de baja granularidad. Por este motivo, a menudo es la métrica de evaluación predeterminada que se usa para modelos genéricos o no especificados que realizan tareas genéricas o no especificadas.
La tasa de verdaderos positivos (TPR), o la proporción de todos los positivos reales que se clasificaron correctamente como positivos, también se conoce como recuperación. Matemáticamente, la recuperación se define de la siguiente manera: Los falsos negativos son positivos reales que se clasificaron erróneamente como negativos, por lo que aparecen en el denominador. En el ejemplo de clasificación de spam, la recuperación mide la fracción de correos electrónicos de spam que se clasificaron correctamente como spam. Por eso, otro nombre para la recuperación es probabilidad de detección: responde a la pregunta “¿Qué fracción de correos electrónicos de spam detecta este modelo?”. Un modelo hipotético perfecto no tendría ningún falso negativo y, por lo tanto, una recuperación (TPR) de 1.0, es decir, una tasa de detección del 100%.
Conozcamos una nueva familia de modelos: Ecuaciones diferenciales ordinarias (ODE, Ordinary Differential Equations). En lugar de especificar una secuencia discreta de capas ocultas, parametrizan la derivada del estado oculto mediante una red neuronal. Los resultados del modelo se calculan utilizando una «caja negra», es decir, el solucionador de ecuaciones diferenciales. Estos modelos de profundidad continua utilizan una cantidad constante de memoria y adaptan su estrategia de estimación a cada señal de entrada. Estos modelos se introdujeron por primera vez en el artículo «Neural Ordinary Differential Equations (Ecuaciones diferenciales ordinarias neuronales)». En este artículo, los autores del método demuestran la capacidad de escalar la retropropagación utilizando cualquier solucionador de ecuaciones diferenciales ordinarias (ODE, por sus siglas en inglés) sin necesidad de acceder a sus operaciones internas. Esto permite el entrenamiento de extremo a extremo de las ODEs dentro de modelos más grandes.
Las redes neuronales gráficas (GNNs, por sus siglas en inglés) han emergido como una poderosa herramienta dentro del campo de la inteligencia artificial, especialmente diseñadas para trabajar con datos estructurados como lo son los grafos, a diferencia de las redes neuronales convencionales, que operan sobre datos Euclidianos como imágenes o secuencias de texto, las GNNs permiten procesar relaciones complejas entre objetos que pueden ser representados de manera más natural en forma de grafos, como en las redes sociales, moléculas químicas o sistemas de recomendación, este tipo de redes lo que tratan de hacer es aprovechar la estructura inherente de los grafos, que son ampliamente utilizados en muchas disciplinas, para obtener representaciones de nodos y relaciones que mejoren tareas como la clasificación, predicción de enlaces, y la recomendación personalizada.
Por primera vez, el trabajo pionero de Lea et al. (2016) propuso una red de convolución basada en el tiempo (TNS) para la segmentación de acción basada en la acción. Este proceso tradicional incluye dos pasos: Primero, CNN calcula características de bajo nivel utilizando (generalmente) información del espacio de codificación; segundo, uso (generalmente) RNN para ingresar estas características de bajo nivel en un clasificador que obtiene información de tiempo avanzada. La principal desventaja de este método es que necesita dos modelos independientes. TCN proporciona un método unificado para capturar los dos niveles de información de manera jerárquica. El marco del codificador codificador se muestra en la Figura 1. Los problemas más críticos son los siguientes: TCN puede aceptar una secuencia de cualquier longitud y producirla a la misma longitud. La convolución causal se usa cuando se usa una estructura de red de convolución completa dimensional. Una característica clave es que la salida de la convolución de T -tiempo con los elementos anteriores de T.
Las Capsule Networks (CapsNets) representan una sofisticada evolución en el campo del aprendizaje profundo (deep learning, DL) diseñada para limitaciones específicas de las redes neuronales convolucionales (CNN) tradicionales. Introducida por primera vez por el prestigioso investigador Geoffrey Hinton y sus colegas, esta arquitectura organiza las neuronas en grupos conocidos como "cápsulas". A diferencia de las neuronas estándar que emiten un único valor de activación escalar, una cápsula emite un vector. La orientación y longitud del vector permiten a la Esta orientación y longitud del vector permiten a la red codificar información más rica sobre un objeto, como su posición exacta, tamaño, orientación y textura. Esta capacidad permite que el modelo comprenda mejor las relaciones jerárquicas entre las características, esencialmente gráficos inversos" para deconstruir una escena visual.
Hablemos del aprendizaje por refuerzo (Reinforcement Learning, en inglés). Ésta es una técnica de inteligencia artificial (IA) en la que un agente inteligente (agent) tiene que interactuar con un entorno (environment), escogiendo una de las acciones (action) que el entorno ofrece en cada uno de los posibles estados (state), e intentar conseguir la mayor recompensa (reward) posible a través de esas acciones. Al principio, el agente no conoce nada sobre el entorno, por lo que tomará acciones de forma aleatoria. Si una acción trae una recompensa positiva, el agente deberá aprender a escoger esa acción más frecuentemente, mientras que si una acción trae una recompensa negativa, el agente deberá aprender a escoger esa acción menos frecuentemente. Así, el agente aprenderá a escoger las acciones que maximicen la suma de recompensas recibidas, también conocida como el retorno (return).
La técnica de Transferencia Neuronal de Estilo de forma resumida, consiste en a partir de dos imágenes, donde una es el contenido y otra es el estilo, combinarlas para generar la misma imagen que en el contenido pero con el estilo de la otra.
¿Qué son las redes neuronales siamesas? Las redes neuronales siamesas son un tipo especial de arquitectura de redes neuronales que se caracterizan por compartir pesos y estructura entre dos o más ramas idénticas. Este enfoque permite comparar y procesar de manera eficiente la similitud entre dos entradas diferentes, lo que resulta especialmente útil en tareas como la verificación de identidad, el reconocimiento de patrones y la detección de anomalías. Funcionamiento de las redes neuronales siamesas Imagina que tienes dos imágenes de rostros diferentes y deseas determinar si pertenecen a la misma persona. Aquí es donde entran en juego las redes neuronales siamesas. Cada imagen se introduce en una rama de la red neuronal siamesa, donde se extraen características relevantes. Posteriormente, estas características se comparan en una capa de similitud para determinar si las dos imágenes son similares o no.
¿Qué es un modelo de difusión? Los modelos de difusión son modelos generativos que se utilizan principalmente para la generación de imágenes y otras tareas de visión artificial. Las redes neuronales basadas en difusión se entrenan mediante deep learning para "difundir" progresivamente muestras con ruido aleatorio y, a continuación, invertir ese proceso de difusión para generar imágenes de alta calidad. Los modelos de difusión se encuentran entre las arquitecturas de redes neuronales que están a la vanguardia de la IA generativa, sobre todo representados por los populares modelos de conversión de texto a imagen, como Stable Diffusion de Stability AI, DALL-E de OpenAI (empieza por DALL-E-2), Midjourney e Imagen de Google. Mejoran el rendimiento y la estabilidad de otras arquitecturas de machine learning utilizadas para la síntesis de imágenes, como los codificadores automáticos variacionales (VAE), las redes generativas de confrontación (GAN) y los modelos autorregresivos, como PixelCNN.
¿Qué es el recorte de degradado? El recorte de gradiente es una técnica utilizada en el entrenamiento de aprendizaje automático modelos, en particular en el contexto del aprendizaje profundo. Aborda el problema de los gradientes explosivos, que pueden ocurrir cuando los gradientes se vuelven excesivamente grandes durante el proceso de retropropagación. Este fenómeno puede provocar un entrenamiento inestable y obstaculizar la convergencia del modelo. Al implementar el recorte de gradientes, los profesionales pueden garantizar que los gradientes permanezcan dentro de un rango específico, estabilizando así el proceso de entrenamiento y mejorando el rendimiento general del modelo.
El suavizado de etiquetas es una técnica de regularización utilizada durante el entrenamiento de modelos de aprendizaje automático para evitar que la red neuronal se vuelva demasiado confiada en sus predicciones. red neuronal se confíe demasiado en sus predicciones. Al modificar ligeramente las etiquetas objetivo, este método este método anima al modelo a producir distribuciones de probabilidad menos extremas, lo que en última generalización y un mejor rendimiento en datos no vistos. Este método mitiga eficazmente el problema común del sobreajuste, en el que un modelo memoriza los datos de entrenamiento en lugar de aprender los subyacentes necesarios para realizar predicciones precisas en situaciones reales.
Stochastic Weight Averaging (SWA) es una técnica que se usa para entrenar modelos de machine learning. Su objetivo es mejorar el rendimiento de un modelo combinando el conocimiento de varios pasos de entrenamiento. En lugar de usar solo el último conjunto de parámetros del modelo, SWA mantiene un promedio en tiempo real de los parámetros del modelo después de un cierto punto en el entrenamiento. Este método puede ayudar al modelo a generalizar mejor a nuevos datos. Sin embargo, hay desafíos con SWA. Aunque puede ayudar, a veces empeora el modelo al hacerlo ajustarse demasiado a los datos de entrenamiento, lo que lleva a un sobreajuste. Por otro lado, si el punto de partida no se elige con cuidado, SWA puede que no aprenda lo suficiente, resultando en un subajuste.
El entrenamiento adversarial es una técnica diseñada para ayudar a los modelos de aprendizaje automático a soportar ataques adversariales. Consiste en entrenar a los modelos usando no solo imágenes normales, sino también estos ejemplos adversariales especialmente diseñados. La idea es que al exponer al modelo a estas imágenes modificadas durante el entrenamiento, puede aprender a identificar y resistir intentos de engañarlo.
SpectralNormalization es una técnica de regularización en Keras que se aplica a capas específicas dentro de un modelo de aprendizaje profundo. Su función principal es controlar la constante de Lipschitz de los pesos de una capa, lo que se traduce en limitar la magnitud de los cambios en la salida de la capa frente a cambios en la entrada. Esto se logra al restringir la norma espectral (el valor singular más grande) de los pesos de la capa. La normalización espectral es especialmente útil en el entrenamiento de Redes Generativas Adversariales (GANs), donde ayuda a estabilizar el proceso de entrenamiento y a mejorar la calidad de las muestras generadas.
Los autocodificadores son redes neuronales para aprendizaje no supervisado que comprimen los datos de entrada en un espacio de baja dimensión (usando un codificador) y luego los reconstruyen (usando un decodificador), entrenando la red para minimizar el error de reconstrucción entre la entrada original y su salida reconstruida. Si la capa oculta es demasiado grande, los autocodificadores pueden simplemente aprender a replicar la entrada perfectamente, funcionando como mapeo de identidad y sin extraer características significativas.
Los autocodificadores de eliminación de ruido abordan esto proporcionando una versión deliberadamente ruidosa o corrompida de la entrada al codificador, pero aún usando la entrada original y limpia para calcular la pérdida. Esto entrena al modelo para aprender características útiles y robustas y reduce la posibilidad de simplemente replicar la entrada.
Los autoencoders básicos con una sola capa oculta aprenden de manera efectiva representaciones comprimidas para conjuntos de datos simples, pero a menudo no son suficientes cuando procesan estructuras de datos complejas. Así como las redes neuronales profundas en el aprendizaje supervisado modelan funciones intrincadas apilando capas, los autoencoders pueden lograr una mayor capacidad al incrementar su profundidad. Este método da lugar a los Autoencoders Apilados, también conocidos como autoencoders profundos. Un autoencoder apilado es esencialmente un autoencoder con múltiples capas ocultas tanto en su componente codificador como en su decodificador. En lugar de una sola transformación desde la entrada hasta el espacio latente y de regreso, los datos pasan por una secuencia de transformaciones.
Una red neuronal residual (también conocida como red residual, ResNet)[1]​ es un modelo de aprendizaje profundo en el que las capas de pesos aprenden funciones residuales con referencia a las entradas de las capas. Una red residual[1]​ es una red con conexiones de salto que realizan mapeos de identidad, fusionadas con las salidas de las capas por adición. Se comporta como una Highway Network (Autopista de la información)[2]​ cuyas puertas se abren mediante pesos de sesgo fuertemente positivo. Esto permite que los modelos de aprendizaje profundo con decenas o cientos de capas se entrenen fácilmente y se aproximen a una mayor precisión al profundizar. Las conexiones de salto de identidad, a menudo denominadas "conexiones residuales", también se utilizan en las redes LSTM (memoria a corto-largo plazo) de 1997,[3]​ los modelos de transformador (por ejemplo, BERT, modelos GPT como ChatGPT), el sistema AlphaGo Zero, el sistema AlphaStar y el sistema AlphaFold
Las redes neuronales recurrentes bidireccionales (BiRNN) son una extensión de las redes neuronales recurrentes convencionales que buscan aprovechar la información contextual de una secuencia de datos de manera más efectiva. La principal ventaja de las BiRNN radica en su capacidad para procesar datos en ambas direcciones, es decir, de izquierda a derecha y de derecha a izquierda. Esto se logra mediante la incorporación de dos capas recurrentes, una para cada dirección, que permiten al modelo tener en cuenta no solo el contexto pasado, sino también el futuro en el análisis de secuencias.
Recordemos un poco: en el algoritmo de Self-Attention se usan 3 matrices entrenadas de los coeficientes de peso (Wq, Wk y Wv). Estas matrices se usan para obtener 3 entidades Query (Solicitud), Key (clave) y Value (Valor). Las dos primeras entidades definen la relación por pares entre los elementos de la secuencia, mientras que la última supone el contexto del elemento analizado. No es un secreto que a veces las situaciones pueden resultar ambiguas. Seguramente, y con aún mayor frecuencia, la misma situación se puede interpretar desde varios puntos de vista. Y desde el punto de vista elegido, las conclusiones pueden resultar absolutamente opuestas. En situaciones así, debemos considerar todas las opciones posibles, y sacar las conclusiones pertinentes solo después de realizar un análisis cuidadoso. Precisamente para resolver tales tareas proponemos la atención multi-cabeza. Aquí, cada "cabeza" tiene su propia opinión, y la decisión se toma tras una votación equilibrada.
Las Redes Neuronales Recurrentes (RNN) son una familia de modelos específicamente diseñados para trabajar con datos secuenciales, como las series temporales. A diferencia de las redes neuronales tradicionales (feedforward), que tratan cada entrada de forma independiente, las RNN incorporan una memoria interna que les permite capturar dependencias entre los elementos de una secuencia. Esto permite al modelo aprovechar la información de los pasos previos para mejorar las predicciones futuras. El bloque fundamental de una RNN es la célula recurrente, que en cada paso temporal recibe dos entradas: el dato actual y el estado oculto anterior (la "memoria" de la red). En cada iteración, el estado oculto se actualiza almacenando la información relevante de la secuencia hasta ese momento. Esta arquitectura permite que las RNN puedan “recordar” tendencias y patrones a lo largo del tiempo.  