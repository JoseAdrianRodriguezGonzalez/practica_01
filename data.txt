El aprendizaje supervisado es una técnica de machine learning que utiliza conjuntos de datos de entrada y salida etiquetados por humanos para entrenar modelos de inteligencia artificial. El modelo entrenado aprende las relaciones subyacentes entre las entradas y las salidas, lo que le permite predecir las salidas correctas basadas en nuevas entradas no etiquetadas del mundo real. Los datos etiquetados consisten en puntos de datos de ejemplo junto con las salidas o respuestas correctas. A medida que los datos de entrada se introducen en el algoritmo de machine learning, este reajusta sus ponderaciones hasta que el modelo se ajuste correctamente. Los datos de entrenamiento etiquetados enseñan explícitamente al modelo a identificar las relaciones entre las características y las etiquetas de datos.  El machine learning supervisado ayuda a las organizaciones a resolver diversos problemas del mundo real a escala, como clasificar el spam o predecir los precios de las acciones. Se puede utilizar para crear modelos de machine learning de alta precisión. Cómo funciona el aprendizaje supervisado El aprendizaje supervisado utiliza un conjunto de datos de entrenamiento etiquetado para comprender las relaciones entre los datos de entrada y salida. Los científicos de datos crean manualmente conjuntos de datos de entrenamiento que contienen datos de entrada junto con las etiquetas correspondientes. El aprendizaje supervisado entrena al modelo para aplicar los resultados correctos a los nuevos datos de entrada en casos de uso del mundo real. Durante el entrenamiento, el algoritmo del modelo procesa grandes conjuntos de datos para explorar posibles correlaciones entre entradas y salidas. A continuación, se evalúa el rendimiento del modelo con datos de prueba para averiguar si se entrenó correctamente. La validación cruzada es el proceso de probar un modelo utilizando una parte diferente del conjunto de datos. 
El análisis de regresión lineal es una técnica de analisis de datos que se utiliza para predecir el valor de una variable en función del valor de otra variable. La variable que desea predecir se llama dependiente. La variable que está utilizando para predecir el valor de la otra variable se llama independiente. Esta forma de análisis estima los coeficientes de la ecuación lineal, involucrando una o más variables independientes que predicen mejor el valor de la variable dependiente. La regresión lineal se ajusta a una línea recta o superficie que minimiza las discrepancias entre los valores de salida previstos y reales. Existen calculadoras de regresión lineal simples que utilizan un método de "mínimos cuadrados" para descubrir la línea de mejor ajuste para un conjunto de datos pareados. Luego, estime el valor de X (variable dependiente) a partir de Y (variable independiente).
¿Qué es la regresión logística? La regresión logística estima la probabilidad de que ocurra un evento, como votar o no votar, en función de un conjunto de datos dado de variables independientes. Este tipo de modelo estadístico (también conocido como modelo logit) se utiliza a menudo para la clasificación y el análisis predictivo. Dado que el resultado es una probabilidad, la variable dependiente está acotada entre 0 y 1. En la regresión logística, se aplica una transformación logit sobre las probabilidades, es decir, la probabilidad de éxito dividida por la probabilidad de fracaso. También se conoce comúnmente como logaritmo de probabilidades, o logaritmo natural de probabilidades, y esta función logística se representa mediante las siguientes fórmulas: Logit(pi) = 1/(1+ exp(-pi)) ln(pi/(1-pi)) = Beta_0 + Beta_1*X_1 + … + B_k*K_k En esta ecuación de regresión logística, logit(pi) es la variable dependiente o de respuesta y x es la variable independiente. El parámetro beta, o coeficiente, en este modelo se estima comúnmente mediante estimación de máxima verosimilitud (EMV). Este método prueba diferentes valores de beta a través de múltiples iteraciones para optimizar el mejor ajuste de las probabilidades logarítmicas. Todas estas iteraciones producen la función de verosimilitud logarítmica y la regresión logística busca maximizar esta función para encontrar la mejor estimación de parámetros. Una vez que se encuentra el coeficiente óptimo (o los coeficientes, si hay más de una variable independiente), las probabilidades condicionales para cada observación se pueden calcular, registrar y sumar para obtener una probabilidad prevista. Para la clasificación binaria, una probabilidad menor que 5 predecirá 0, mientras que una probabilidad mayor que 0 predecirá 1. Una vez calculado el modelo, la mejor práctica consiste en evaluar en qué medida el modelo predice la variable dependiente, lo que se denomina bondad de ajuste.
¿Qué es un árbol de decisión? Un árbol de decisión es un algoritmo de aprendizaje supervisado no paramétrico, que se utiliza tanto para tareas de clasificación como de regresión. Tiene una estructura jerárquica de árbol, que consta de un nodo raíz, ramas, nodos internos y nodos hoja. Como puede ver en el siguiente diagrama, un árbol de decisión comienza con un nodo raíz, que no tiene ninguna rama entrante. Las ramas salientes del nodo raíz luego alimentan los nodos internos, también conocidos como nodos de decisión. En función de las características disponibles, ambos tipos de nodos realizan evaluaciones para formar subconjuntos homogéneos, que se denotan mediante nodos hoja o nodos terminales. Los nodos hoja representan todos los resultados posibles dentro del conjunto de datos.
¿Qué es el algoritmo KNN? El algoritmo KNN (k-nearest neighbors) es un clasificador de aprendizaje supervisado no paramétrico, que emplea la proximidad para realizar clasificaciones o predicciones sobre la agrupación de un punto de datos individual. Es uno de los clasificadores de clasificación y regresión más populares y sencillos que se emplean actualmente en machine learning. Si bien el algoritmo KNN se puede usar para problemas de regresión o clasificación, generalmente se usa como un algoritmo de clasificación, partiendo del supuesto de que se pueden encontrar puntos similares cerca uno del otro. En los problemas de clasificación, la etiqueta de clase se asigna por mayoría de votos, es decir, se emplea la etiqueta que se representa con más frecuencia en torno a un punto de datos determinado. Aunque técnicamente se considera "votación por pluralidad", el término "votación por mayoría" se emplea más comúnmente en las publicaciones al respecto. La distinción entre estas terminologías es que la "votación por mayoría" requiere técnicamente una mayoría superior al 50%, lo que funciona sobre todo cuando sólo hay dos categorías. Cuando hay varias clases, por ejemplo, cuatro categorías, usted no requiere necesariamente el 50% de los votos para llegar a una conclusión sobre una clase, ya que podría asignar una etiqueta de clase con un voto superior al 25%
¿Qué son las SVM? Una máquina de vectores de soporte (SVM) es un algoritmo de aprendizaje automático supervisado que clasifica los datos al encontrar una línea o hiperplano óptimo que maximice la distancia entre cada clase en un espacio N-dimensional. Las SVM fueron desarrolladas en la década de 1990 por Vladimir N. Vapnik y sus colegas, y publicaron este trabajo en un artículo titulado "Support Vector Method for Function Approximation, Regression Estimation, and Signal Processing"1 en 1995. Las SVM se emplean comúnmente en problemas de clasificación. Distinguen entre dos clases encontrando el hiperplano óptimo que maximiza el margen entre los puntos de datos más cercanos de clases opuestas. El número de características en los datos de entrada determina si el hiperplano es una línea en un espacio bidimensional o un plano en un espacio n-dimensional. Dado que se pueden encontrar múltiples hiperplanos para diferenciar clases, la maximización del margen entre puntos permite al algoritmo encontrar la mejor frontera de decisión entre clases. Esto, a su vez, le permite generalizar bien nuevos datos y hacer predicciones de clasificación precisas. Las líneas adyacentes al hiperplano óptimo se conocen como vectores de soporte, ya que estos vectores atraviesan los puntos de datos que determinan el margen máximo. El algoritmo SVM se emplea ampliamente en el aprendizaje automático ya que puede manejar tareas de clasificación tanto lineales como no lineales. Sin embargo, cuando los datos no son separables linealmente, las funciones del kernel se utilizan para transformar el espacio multidimensional de los datos y permitir la separación lineal.
¿Qué es el bosque aleatorio? El bosque aleatorio es un algoritmo de aprendizaje automático de uso común, registrado por Leo Breiman y Adele Cutler, que combina el resultado de múltiples árboles de decisión para llegar a un resultado único. Su facilidad de uso y flexibilidad han impulsado su adopción, ya que maneja problemas de clasificación y regresión. Árboles de decisión Dado que el modelo de bosque aleatorio se compone de varios árboles de decisión, sería útil empezar describiendo brevemente el algoritmo del árbol de decisión. Los árboles de decisión comienzan con una pregunta básica, como "¿Debería navegar?" A partir de ahí, puede hacer una serie de preguntas para determinar una respuesta, como "¿El oleaje es prolongado?" o "¿Hay viento en alta mar?". Estas preguntas constituyen los nodos de decisión en el árbol, que funcionan como un medio para dividir los datos. Cada pregunta ayuda a un individuo a llegar a una decisión final, que sería señalada por el nodo hoja. Las observaciones que cumplan con los criterios seguirán el ramal "Sí" y las que no, seguirán la ruta alternativa. Los árboles de decisión buscan encontrar la mejor división para los subconjuntos de datos y, por lo general, se entrenan a través del algoritmo del árbol de clasificación y regresión (CART). Las métricas, como la impureza de Gini, la ganancia de información o el error cuadrático medio (MSE), pueden utilizarse para evaluar la calidad de la división. Este árbol de decisión es un ejemplo de un problema de clasificación, donde las etiquetas de clase son "navegar" y "no navegar". Si bien los árboles de decisión son algoritmos comunes de aprendizaje supervisado, pueden ser proclives a presentar problemas, como sesgos y sobreajuste. Sin embargo, cuando varios árboles de decisión forman un conjunto en el algoritmo de bosque aleatorio, predicen resultados más precisos, en especial cuando los árboles de decisión individuales no están correlacionados entre sí.
Funcionamiento de las redes CNN Las redes neuronales convolucionales pueden tener decenas o cientos de capas, y cada una de ellas aprende a detectar diferentes características de una imagen. Se aplican filtros a las imágenes de entrenamiento con distintas resoluciones, y la salida resultante de convolucionar cada imagen se emplea como entrada para la siguiente capa. Los filtros pueden comenzar como características muy simples, tales como brillo y bordes, e ir creciendo en complejidad hasta convertirse en características que definen el objeto de forma singular. Aprendizaje de características, capas y clasificación Una CNN consta de una capa de entrada, una capa de salida y varias capas ocultas entre ambas. Estas capas realizan operaciones que modifican los datos, con el propósito de comprender sus características particulares. Las 3 capas más comunes son: convolución, activación o ReLU, y agrupación. Convolución: Aplica un conjunto de filtros convolucionales a las imágenes de entrada; cada filtro activa diferentes características de las imágenes. Unidad lineal rectificada (ReLU): Mantiene los valores positivos y establece los valores negativos en cero, lo que permite un entrenamiento más rápido y eficaz. También se lo conoce como activación, ya que solo las características activadas prosiguen a la siguiente capa. Agrupación: Simplifica la salida mediante reducción no lineal de la tasa de muestreo, lo que disminuye el número de parámetros que la red debe aprender.
¿Qué son los clasificadores Naive Bayes? El clasificador Naive Bayes es un algoritmo de machine learning supervisado que se utiliza para tareas de clasificación como la clasificación de textos. Utiliza principios de probabilidad para realizar tareas de clasificación. Naïve Bayes forma parte de una familia de algoritmos de aprendizaje generativo, lo que significa que busca modelar la distribución de las entradas de una clase o categoría determinada. A diferencia de los clasificadores discriminativos, como la regresión logística, no aprende qué características son las más importantes para diferenciar entre clases. Breve repaso a la estadística bayesiana El Naïve Bayes también se conoce como clasificador probabilístico, ya que se basa en el teorema de Bayes. Sería difícil explicar este algoritmo sin explicar los fundamentos de la estadística bayesiana. Este teorema, también conocido como regla de Bayes, nos permite "invertir" las probabilidades condicionales. Como recordatorio, las probabilidades condicionales representan la probabilidad de que un evento se produzca dado que se ha producido algún otro evento, lo que se representa con la siguiente fórmula:
¿Qué es ANN en IA? Una red neuronal artificial o artificial neural network (ANN) es un modelo matemático que imita el procesamiento de información de una red neuronal biológica. Está compuesta por nodos interconectados, que simulan las neuronas biológicas, y capas, que representan las conexiones entre estas neuronas. Cada nodo tiene un peso y un valor de umbral, y cuando la salida de un nodo supera el umbral, se activa y envía su información a la siguiente capa de la red. Las redes neuronales artificiales son capaces de aprender y mejorar su precisión a medida que se les suministra más información de entrenamiento. Utilizan algoritmos de aprendizaje automático para ajustar los pesos y los umbrales de los nodos, de manera que la red pueda realizar tareas específicas de manera más eficiente y precisa. El aprendizaje automático como base de las ANN El aprendizaje automático es un subconjunto de la inteligencia artificial que permite la optimización de los algoritmos. A través del aprendizaje automático, las redes neuronales artificiales pueden hacer predicciones y minimizar los errores derivados de la simple suposición. Por ejemplo, empresas como Amazon utilizan el aprendizaje automático para recomendar productos a sus clientes en función de sus preferencias y comportamientos anteriores. El aprendizaje automático se divide en varias categorías, como el aprendizaje supervisado, el aprendizaje no supervisado, el aprendizaje por refuerzo y el aprendizaje en línea. Cada una de estas categorías tiene sus propias características y aplicaciones específicas. 
¿Qué es una red neuronal recurrente? Una red neuronal recurrente o RNN es una red neuronal profunda que se entrena con datos secuenciales o de series temporales para crear un modelo de machine learning (ML) que pueda hacer predicciones o conclusiones secuenciales basándose en entradas secuenciales. Una RNN podría utilizarse para predecir los niveles diarios de inundación basándose en los datos diarios anteriores sobre inundaciones, mareas y meteorología. Pero las RNN también se pueden utilizar para resolver problemas ordinales o temporales, como la traducción de idiomas, el procesamiento del lenguaje natural (PLN), el análisis de sentimientos, el reconocimiento de voz y el subtitulado de imágenes. Cómo funcionan las RNN Al igual que las redes neuronales tradicionales, como las redes neuronales prealimentadas y las redes neuronales convolucionales (CNN), las redes neuronales recurrentes utilizan los datos de entrenamiento para aprender. Se distinguen por su "memoria", puesto que toman la información de las entradas anteriores para influir en la entrada y la salida actuales. Mientras que las redes tradicionales de deep learning asumen que las entradas y las salidas son independientes entre sí, las salidas de las redes neuronales recurrentes depende de los elementos anteriores dentro de la secuencia. Mientras que las redes neuronales profundas tradicionales asumen que las entradas y las salidas son independientes entre sí, la salida de las RNN depende de los elementos anteriores dentro de la secuencia.
La memoria larga a corto plazo, (en inglés, long short-term memory o LSTM) es una arquitectura de red neuronal recurrente (RNN) desarrollada para abordar el problema de desvanecimiento de gradiente,[1] que afecta la capacidad de las RNN tradicionales para aprender dependencias a largo plazo en secuencias de datos. A diferencia de otros modelos como los modelos ocultos de Markov, los LSTM pueden retener información durante miles de pasos temporales, lo que los convierte en una herramienta eficaz para procesar datos secuenciales.[2] El núcleo de un LSTM es la célula de memoria, que retiene información durante intervalos de tiempo arbitrarios. Esta célula está controlada por tres tipos de puertas: la puerta de entrada, la puerta de salida[3] y la puerta de olvido.[4] La puerta de entrada regula qué nueva información se almacenará en la célula de memoria, la puerta de salida decide qué información se usará para generar la salida en el paso actual, y la puerta de olvido determina qué información debe eliminarse.[2]​
¿Qué es la reducción de la dimensionalidad? Las técnicas de reducción de la dimensionalidad como PCA, LDA y t-SNE mejoran los modelos de machine learning. Conservan características esenciales de conjuntos de datos complejos al reducir el número de variables de previsión para una mayor generalización. La reducción de la dimensionalidad es un método para representar un conjunto de datos utilizando un menor número de características (es decir, dimensiones) sin perder las propiedades significativas de los datos originales.1 Esto equivale a eliminar características irrelevantes o redundantes, o simplemente datos ruidosos, para crear un modelo con un menor número de variables. La reducción de la dimensionalidad abarca una serie de métodos de selección de características y compresión de datos utilizados durante el preprocesamiento. Aunque los métodos de reducción de la dimensionalidad difieren en su funcionamiento, todos transforman espacios de alta dimensión en espacios de baja dimensión mediante la extracción o la combinación de variables.
El agrupamiento espacial basado en densidad de aplicaciones con ruido o Density-based spatial clustering of applications with noise (DBSCAN) es un algoritmo de agrupamiento de datos (data clustering) propuesto por Martin Ester, Hans-Peter Kriegel, Jörg Sander y Xiaowei Xu en 1996.[1]Es un algoritmo de agrupamiento basado en densidad (density-based clustering) porque encuentra un número de grupos (clusters) comenzando por una estimación de la distribución de densidad de los nodos correspondientes. DBSCAN es uno de los algoritmos de agrupamiento más usados y citados en la literatura científica.[2] OPTICS puede verse como una generalización de DBSCAN para múltiples rangos, reemplazando el parámetro e por el radio máximo de búsqueda. En 2014, el algoritmo fue merecedor del premio a la prueba del tiempo (un reconocimiento dado a algoritmos que han recibido una sustancial atención en la teoría y la práctica) en la conferencia líder de la minería de datos, KDD.[3] El paper "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN" aparece en la lista de los 8 artículos más descargados en la revista ACM Transactions on Database Systems (TODS).
La agrupación de medias K es un algoritmo de aprendizaje no supervisado utilizado para la agrupación en clústeres de datos, que agrupa puntos de datos no etiquetados en grupos o clústeres. Es uno de los métodos de clustering más populares utilizados en el machine learning. A diferencia del aprendizaje supervisado, los datos de entrenamiento que utiliza este algoritmo no están etiquetados, lo que significa que los puntos de datos no tienen una estructura de clasificación definida. Aunque existen varios tipos de algoritmos de clustering, incluidos los exclusivos, los superpuestos, los jerárquicos y los probabilísticos, el algoritmo de clustering de medias k es un ejemplo de método de clustering exclusivo o "duro". Esta forma de agrupación estipula que un punto de datos puede existir en un solo clúster. Este tipo de análisis de clústeres se utiliza habitualmente en la ciencia de datos para la segmentación de mercados, la agrupación de documentos, la segmentación de imágenes y la compresión de imágenes. El algoritmo de medias k es un método ampliamente utilizado en el análisis de clústeres porque es eficiente, eficaz y sencillo. Las medias K son un algoritmo de clustering iterativo basado en centroides que divide un conjunto de datos en grupos similares en función de la distancia entre sus centroides. El centroide, o centro del clúster, es la media o la mediana de todos los puntos dentro del clúster, según las características de los datos.
BERT (Bidirectional Encoder Representations from Transformers) o Representación de Codificador Bidireccional de Transformadores es una técnica basada en redes neuronales para el pre-entrenamiento del procesamiento del lenguaje natural (PLN) desarrollada por Google.[1] BERT fue creado y publicado en 2018 por Jacob Devlin y sus compañeros en Google[2][3]y su uso original fue para comprender mejor las búsquedas de las personas usuarias.[4] Lo que distingue a BERT es su capacidad para captar el contexto bidireccional en una oración. Es decir, que puede captar el significado de una palabra en relación con las palabras que la rodean. Esta mejora en la comprensión contextual ha llevado a un mayor rendimiento en diversas tareas de procesamiento del lenguaje natural. Lo cual ha sido aprovechado por Google para perfeccionar la comprensión de las búsquedas.[5]​ La implementación de BERT ha llevado a resultados de búsqueda más precisos y relevantes, ya que el motor de búsqueda puede entender mejor el contexto detrás de las consultas.
¿Qué es el PLN? El procesamiento del lenguaje natural (PLN) es un subcampo de la informática y la inteligencia artificial (IA) que utiliza el machine learning para permitir que los ordenadores entiendan y se comuniquen con el lenguaje humano. El PLN permite a los ordenadores y dispositivos digitales reconocer, comprender y generar texto y voz combinando la lingüística computacional (el modelado del lenguaje humano basado en reglas) junto con el modelado estadístico, el machine learning y el deep learning. La investigación en PLN ha ayudado a hacer posible la era de la IA generativa, desde las habilidades de comunicación de los modelos de lenguaje de gran tamaño (LLM) hasta la capacidad de los modelos de generación de imágenes para comprender las solicitudes. El PLN ya forma parte de la vida cotidiana de muchas personas, como los motores de búsqueda, los chatbots del servicio de atención al cliente con comandos hablados, los sistemas GPS controlados por voz y los asistentes digitales para responder preguntas en los smartphones, como Alexa de Amazon, Siri de Apple y Cortana de Microsoft.
ARIMA significa media móvil integrada autorregresiva y es una técnica para el análisis de series temporales y para pronosticar posibles valores futuros de una serie temporal. El modelado autorregresivo y el modelado de media móvil son dos enfoques diferentes para pronosticar datos de series temporales. ARIMA integra estos dos enfoques, de ahí el nombre. La Forecasting es una rama del machine learning que emplea el comportamiento pasado de una serie temporal para predecir uno o más valores futuros de esa serie temporal. Imagine que está comprando helado para abastecer una pequeña tienda. Si sabe que las ventas de helado aumentan constantemente a medida que el clima se calienta, probablemente debería predecir que el pedido de la próxima semana debería ser un poco mayor que el pedido de esta semana. El aumento dependerá de la diferencia entre las ventas de esta semana y las de la anterior No podemos pronosticar el futuro sin un pasado para compararlo, por lo que los datos de series temporales pasadas son muy importantes para ARIMA y para todos los métodos de forecasting y análisis de series temporales.
La exactitud es la proporción de todas las clasificaciones que fueron correctas, ya sean positivas o negativas. Se define matemáticamente de la siguiente manera: En el ejemplo de clasificación de spam, la precisión mide la fracción de todos los correos electrónicos clasificados correctamente. Un modelo perfecto no tendría ningún falso positivo ni ningún falso negativo y, por lo tanto, tendría una precisión de 1.0 o 100%. Debido a que incorpora los cuatro resultados de la matriz de confusión (VP, FP, TN y FN), dado un conjunto de datos equilibrado, con cantidades similares de ejemplos en ambas clases, la precisión puede servir como una medida de calidad del modelo de baja granularidad. Por este motivo, a menudo es la métrica de evaluación predeterminada que se usa para modelos genéricos o no especificados que realizan tareas genéricas o no especificadas.
La tasa de verdaderos positivos (TPR), o la proporción de todos los positivos reales que se clasificaron correctamente como positivos, también se conoce como recuperación. Matemáticamente, la recuperación se define de la siguiente manera: Los falsos negativos son positivos reales que se clasificaron erróneamente como negativos, por lo que aparecen en el denominador. En el ejemplo de clasificación de spam, la recuperación mide la fracción de correos electrónicos de spam que se clasificaron correctamente como spam. Por eso, otro nombre para la recuperación es probabilidad de detección: responde a la pregunta “¿Qué fracción de correos electrónicos de spam detecta este modelo?”. Un modelo hipotético perfecto no tendría ningún falso negativo y, por lo tanto, una recuperación (TPR) de 1.0, es decir, una tasa de detección del 100%.
Conozcamos una nueva familia de modelos: Ecuaciones diferenciales ordinarias (ODE, Ordinary Differential Equations). En lugar de especificar una secuencia discreta de capas ocultas, parametrizan la derivada del estado oculto mediante una red neuronal. Los resultados del modelo se calculan utilizando una «caja negra», es decir, el solucionador de ecuaciones diferenciales. Estos modelos de profundidad continua utilizan una cantidad constante de memoria y adaptan su estrategia de estimación a cada señal de entrada. Estos modelos se introdujeron por primera vez en el artículo «Neural Ordinary Differential Equations (Ecuaciones diferenciales ordinarias neuronales)». En este artículo, los autores del método demuestran la capacidad de escalar la retropropagación utilizando cualquier solucionador de ecuaciones diferenciales ordinarias (ODE, por sus siglas en inglés) sin necesidad de acceder a sus operaciones internas. Esto permite el entrenamiento de extremo a extremo de las ODEs dentro de modelos más grandes.
Las redes neuronales gráficas (GNNs, por sus siglas en inglés) han emergido como una poderosa herramienta dentro del campo de la inteligencia artificial, especialmente diseñadas para trabajar con datos estructurados como lo son los grafos, a diferencia de las redes neuronales convencionales, que operan sobre datos Euclidianos como imágenes o secuencias de texto, las GNNs permiten procesar relaciones complejas entre objetos que pueden ser representados de manera más natural en forma de grafos, como en las redes sociales, moléculas químicas o sistemas de recomendación, este tipo de redes lo que tratan de hacer es aprovechar la estructura inherente de los grafos, que son ampliamente utilizados en muchas disciplinas, para obtener representaciones de nodos y relaciones que mejoren tareas como la clasificación, predicción de enlaces, y la recomendación personalizada.
Por primera vez, el trabajo pionero de Lea et al. (2016) propuso una red de convolución basada en el tiempo (TNS) para la segmentación de acción basada en la acción. Este proceso tradicional incluye dos pasos: Primero, CNN calcula características de bajo nivel utilizando (generalmente) información del espacio de codificación; segundo, uso (generalmente) RNN para ingresar estas características de bajo nivel en un clasificador que obtiene información de tiempo avanzada. La principal desventaja de este método es que necesita dos modelos independientes. TCN proporciona un método unificado para capturar los dos niveles de información de manera jerárquica. El marco del codificador codificador se muestra en la Figura 1. Los problemas más críticos son los siguientes: TCN puede aceptar una secuencia de cualquier longitud y producirla a la misma longitud. La convolución causal se usa cuando se usa una estructura de red de convolución completa dimensional. Una característica clave es que la salida de la convolución de T -tiempo con los elementos anteriores de T.
Las Capsule Networks (CapsNets) representan una sofisticada evolución en el campo del aprendizaje profundo (deep learning, DL) diseñada para limitaciones específicas de las redes neuronales convolucionales (CNN) tradicionales. Introducida por primera vez por el prestigioso investigador Geoffrey Hinton y sus colegas, esta arquitectura organiza las neuronas en grupos conocidos como "cápsulas". A diferencia de las neuronas estándar que emiten un único valor de activación escalar, una cápsula emite un vector. La orientación y longitud del vector permiten a la Esta orientación y longitud del vector permiten a la red codificar información más rica sobre un objeto, como su posición exacta, tamaño, orientación y textura. Esta capacidad permite que el modelo comprenda mejor las relaciones jerárquicas entre las características, esencialmente gráficos inversos" para deconstruir una escena visual.
Hablemos del aprendizaje por refuerzo (Reinforcement Learning, en inglés). Ésta es una técnica de inteligencia artificial (IA) en la que un agente inteligente (agent) tiene que interactuar con un entorno (environment), escogiendo una de las acciones (action) que el entorno ofrece en cada uno de los posibles estados (state), e intentar conseguir la mayor recompensa (reward) posible a través de esas acciones. Al principio, el agente no conoce nada sobre el entorno, por lo que tomará acciones de forma aleatoria. Si una acción trae una recompensa positiva, el agente deberá aprender a escoger esa acción más frecuentemente, mientras que si una acción trae una recompensa negativa, el agente deberá aprender a escoger esa acción menos frecuentemente. Así, el agente aprenderá a escoger las acciones que maximicen la suma de recompensas recibidas, también conocida como el retorno (return).
La técnica de Transferencia Neuronal de Estilo de forma resumida, consiste en a partir de dos imágenes, donde una es el contenido y otra es el estilo, combinarlas para generar la misma imagen que en el contenido pero con el estilo de la otra.
¿Qué son las redes neuronales siamesas? Las redes neuronales siamesas son un tipo especial de arquitectura de redes neuronales que se caracterizan por compartir pesos y estructura entre dos o más ramas idénticas. Este enfoque permite comparar y procesar de manera eficiente la similitud entre dos entradas diferentes, lo que resulta especialmente útil en tareas como la verificación de identidad, el reconocimiento de patrones y la detección de anomalías. Funcionamiento de las redes neuronales siamesas Imagina que tienes dos imágenes de rostros diferentes y deseas determinar si pertenecen a la misma persona. Aquí es donde entran en juego las redes neuronales siamesas. Cada imagen se introduce en una rama de la red neuronal siamesa, donde se extraen características relevantes. Posteriormente, estas características se comparan en una capa de similitud para determinar si las dos imágenes son similares o no.
¿Qué es un modelo de difusión? Los modelos de difusión son modelos generativos que se utilizan principalmente para la generación de imágenes y otras tareas de visión artificial. Las redes neuronales basadas en difusión se entrenan mediante deep learning para "difundir" progresivamente muestras con ruido aleatorio y, a continuación, invertir ese proceso de difusión para generar imágenes de alta calidad. Los modelos de difusión se encuentran entre las arquitecturas de redes neuronales que están a la vanguardia de la IA generativa, sobre todo representados por los populares modelos de conversión de texto a imagen, como Stable Diffusion de Stability AI, DALL-E de OpenAI (empieza por DALL-E-2), Midjourney e Imagen de Google. Mejoran el rendimiento y la estabilidad de otras arquitecturas de machine learning utilizadas para la síntesis de imágenes, como los codificadores automáticos variacionales (VAE), las redes generativas de confrontación (GAN) y los modelos autorregresivos, como PixelCNN.
¿Qué es el recorte de degradado? El recorte de gradiente es una técnica utilizada en el entrenamiento de aprendizaje automático modelos, en particular en el contexto del aprendizaje profundo. Aborda el problema de los gradientes explosivos, que pueden ocurrir cuando los gradientes se vuelven excesivamente grandes durante el proceso de retropropagación. Este fenómeno puede provocar un entrenamiento inestable y obstaculizar la convergencia del modelo. Al implementar el recorte de gradientes, los profesionales pueden garantizar que los gradientes permanezcan dentro de un rango específico, estabilizando así el proceso de entrenamiento y mejorando el rendimiento general del modelo.
El suavizado de etiquetas es una técnica de regularización utilizada durante el entrenamiento de modelos de aprendizaje automático para evitar que la red neuronal se vuelva demasiado confiada en sus predicciones. red neuronal se confíe demasiado en sus predicciones. Al modificar ligeramente las etiquetas objetivo, este método este método anima al modelo a producir distribuciones de probabilidad menos extremas, lo que en última generalización y un mejor rendimiento en datos no vistos. Este método mitiga eficazmente el problema común del sobreajuste, en el que un modelo memoriza los datos de entrenamiento en lugar de aprender los subyacentes necesarios para realizar predicciones precisas en situaciones reales.
Stochastic Weight Averaging (SWA) es una técnica que se usa para entrenar modelos de machine learning. Su objetivo es mejorar el rendimiento de un modelo combinando el conocimiento de varios pasos de entrenamiento. En lugar de usar solo el último conjunto de parámetros del modelo, SWA mantiene un promedio en tiempo real de los parámetros del modelo después de un cierto punto en el entrenamiento. Este método puede ayudar al modelo a generalizar mejor a nuevos datos. Sin embargo, hay desafíos con SWA. Aunque puede ayudar, a veces empeora el modelo al hacerlo ajustarse demasiado a los datos de entrenamiento, lo que lleva a un sobreajuste. Por otro lado, si el punto de partida no se elige con cuidado, SWA puede que no aprenda lo suficiente, resultando en un subajuste.
El entrenamiento adversarial es una técnica diseñada para ayudar a los modelos de aprendizaje automático a soportar ataques adversariales. Consiste en entrenar a los modelos usando no solo imágenes normales, sino también estos ejemplos adversariales especialmente diseñados. La idea es que al exponer al modelo a estas imágenes modificadas durante el entrenamiento, puede aprender a identificar y resistir intentos de engañarlo.
SpectralNormalization es una técnica de regularización en Keras que se aplica a capas específicas dentro de un modelo de aprendizaje profundo. Su función principal es controlar la constante de Lipschitz de los pesos de una capa, lo que se traduce en limitar la magnitud de los cambios en la salida de la capa frente a cambios en la entrada. Esto se logra al restringir la norma espectral (el valor singular más grande) de los pesos de la capa. La normalización espectral es especialmente útil en el entrenamiento de Redes Generativas Adversariales (GANs), donde ayuda a estabilizar el proceso de entrenamiento y a mejorar la calidad de las muestras generadas.
Los autocodificadores son redes neuronales para aprendizaje no supervisado que comprimen los datos de entrada en un espacio de baja dimensión (usando un codificador) y luego los reconstruyen (usando un decodificador), entrenando la red para minimizar el error de reconstrucción entre la entrada original y su salida reconstruida. Si la capa oculta es demasiado grande, los autocodificadores pueden simplemente aprender a replicar la entrada perfectamente, funcionando como mapeo de identidad y sin extraer características significativas.
Los autocodificadores de eliminación de ruido abordan esto proporcionando una versión deliberadamente ruidosa o corrompida de la entrada al codificador, pero aún usando la entrada original y limpia para calcular la pérdida. Esto entrena al modelo para aprender características útiles y robustas y reduce la posibilidad de simplemente replicar la entrada.
Los autoencoders básicos con una sola capa oculta aprenden de manera efectiva representaciones comprimidas para conjuntos de datos simples, pero a menudo no son suficientes cuando procesan estructuras de datos complejas. Así como las redes neuronales profundas en el aprendizaje supervisado modelan funciones intrincadas apilando capas, los autoencoders pueden lograr una mayor capacidad al incrementar su profundidad. Este método da lugar a los Autoencoders Apilados, también conocidos como autoencoders profundos. Un autoencoder apilado es esencialmente un autoencoder con múltiples capas ocultas tanto en su componente codificador como en su decodificador. En lugar de una sola transformación desde la entrada hasta el espacio latente y de regreso, los datos pasan por una secuencia de transformaciones.
Una red neuronal residual (también conocida como red residual, ResNet)[1] es un modelo de aprendizaje profundo en el que las capas de pesos aprenden funciones residuales con referencia a las entradas de las capas. Una red residual[1] es una red con conexiones de salto que realizan mapeos de identidad, fusionadas con las salidas de las capas por adición. Se comporta como una Highway Network (Autopista de la información)[2] cuyas puertas se abren mediante pesos de sesgo fuertemente positivo. Esto permite que los modelos de aprendizaje profundo con decenas o cientos de capas se entrenen fácilmente y se aproximen a una mayor precisión al profundizar. Las conexiones de salto de identidad, a menudo denominadas "conexiones residuales", también se utilizan en las redes LSTM (memoria a corto-largo plazo) de 1997,[3] los modelos de transformador (por ejemplo, BERT, modelos GPT como ChatGPT), el sistema AlphaGo Zero, el sistema AlphaStar y el sistema AlphaFold
Las redes neuronales recurrentes bidireccionales (BiRNN) son una extensión de las redes neuronales recurrentes convencionales que buscan aprovechar la información contextual de una secuencia de datos de manera más efectiva. La principal ventaja de las BiRNN radica en su capacidad para procesar datos en ambas direcciones, es decir, de izquierda a derecha y de derecha a izquierda. Esto se logra mediante la incorporación de dos capas recurrentes, una para cada dirección, que permiten al modelo tener en cuenta no solo el contexto pasado, sino también el futuro en el análisis de secuencias.
Recordemos un poco: en el algoritmo de Self-Attention se usan 3 matrices entrenadas de los coeficientes de peso (Wq, Wk y Wv). Estas matrices se usan para obtener 3 entidades Query (Solicitud), Key (clave) y Value (Valor). Las dos primeras entidades definen la relación por pares entre los elementos de la secuencia, mientras que la última supone el contexto del elemento analizado. No es un secreto que a veces las situaciones pueden resultar ambiguas. Seguramente, y con aún mayor frecuencia, la misma situación se puede interpretar desde varios puntos de vista. Y desde el punto de vista elegido, las conclusiones pueden resultar absolutamente opuestas. En situaciones así, debemos considerar todas las opciones posibles, y sacar las conclusiones pertinentes solo después de realizar un análisis cuidadoso. Precisamente para resolver tales tareas proponemos la atención multi-cabeza. Aquí, cada "cabeza" tiene su propia opinión, y la decisión se toma tras una votación equilibrada.
Las Redes Neuronales Recurrentes (RNN) son una familia de modelos específicamente diseñados para trabajar con datos secuenciales, como las series temporales. A diferencia de las redes neuronales tradicionales (feedforward), que tratan cada entrada de forma independiente, las RNN incorporan una memoria interna que les permite capturar dependencias entre los elementos de una secuencia. Esto permite al modelo aprovechar la información de los pasos previos para mejorar las predicciones futuras. El bloque fundamental de una RNN es la célula recurrente, que en cada paso temporal recibe dos entradas: el dato actual y el estado oculto anterior (la "memoria" de la red). En cada iteración, el estado oculto se actualiza almacenando la información relevante de la secuencia hasta ese momento. Esta arquitectura permite que las RNN puedan “recordar” tendencias y patrones a lo largo del tiempo. 
Un algoritmo genético (AG) es un método para solucionar problemas de optimización con o sin restricciones basándose en un proceso de selección natural que imita la evolución biológica. Este algoritmo modifica repetidamente una población de soluciones individuales. En cada paso, el algoritmo genético selecciona individuos de la población actual aleatoriamente y los utiliza como padres para producir los hijos de la siguiente generación. Tras varias generaciones sucesivas, la población "evoluciona" hacia una solución óptima.
La programación genética (PG) es un método especializado dentro de los algoritmos evolutivos, un subconjunto de técnicas de aprendizaje automático inspiradas en la selección natural y la supervivencia del más apto. La PG permite a las computadoras resolver problemas automáticamente y generar nuevos programas informáticos imitando operaciones genéticas naturales, como cruces, mutaciones , reproducciones, duplicaciones y deleciones de genes. A diferencia de los algoritmos genéticos , que representan soluciones como cadenas de números de longitud fija, GP representa los programas como estructuras de árbol que codifican su sintaxis, lo que permite su creación y modificación mediante operaciones genéticas. El algoritmo evalúa estos programas, conservando solo aquellos que realizan una tarea o alcanzan un objetivo específico, y modifica y combina los programas restantes para crear nuevos candidatos para su evaluación.
Las estrategias evolutivas forman una de las corrientes de la computación evolutiva, la cual se utiliza para el problema particular de maximizar(o minimizar) una función real. Los operadores genéticos que se utilizan son los mismos que en los algoritmos genéticos, pero la importancia de cada uno de ellos varía. En las estrategias evolutivas el operador más importante es la mutación.
El algoritmo de Evolución Diferencial (ED) es un tipo de algoritmo evolutivo, propuesto por primera vez por Storn y Price en 1995. El punto en común entre ED y otros algoritmos evolutivos es que son algoritmos basados en poblaciones que abarcan los siguientes procedimientos: cruce, mutación y selección. Entre los procedimientos básicos de ED, el proceso de mutación y el paso de selección difieren de otros algoritmos evolutivos. En comparación con otros algoritmos evolutivos, ED presenta las siguientes ventajas: estructura simple, convergencia rápida, facilidad de uso, velocidad y robustez. En consecuencia, ED se considera un algoritmo potencial para resolver problemas de optimización. Los procedimientos específicos se muestran a continuación:
La Optimización por Enjambres de Partículas es una técnica de optimización/búsqueda. Aunque normalmente PSO se usa en espacios de búsqueda con muchas dimensiones, los ejemplos que se mostrarán aquí harán uso de un espacio bidimensional, con el fin de facilitar la visualización, y porque nuestro objetivo es puramente didáctico, esperando que el interesado no encuentre dificultades en extenderlo a otros casos (la librería proporcionada en el repositorio de la asignatura se adapta al número de dimensiones que se necesite). Este método fue descrito alrededor de 1995 por Kennedy y Eberhart, y se inspira en el comportamiento de los enjambres de insectos en la naturaleza. En concreto, podemos pensar en un enjambre de abejas, ya que éstas a la hora de recolectar polen buscan la región del espacio en la que existe más densidad de flores, porque la probabilidad de que haya polen es mayor. La misma idea fue trasladada al campo de la computación en forma de algoritmo y se emplea en la actualidad en la optimización de distintos tipos de sistemas. 
Los algoritmos Meméticos es un algoritmo poblacional, que puede verse como una variante de los algoritmos genéticos.La idea básica del algoritmo es la de incorporar la mayor cantidad de conocimiento del dominio que sea posible durante el proceso de generación de una nueva población. Así como en búsqueda local teniamos definida una vecindad para un solo individuo, la vecindad de una población de individuos se puede obtener mediante la composición de los individuos. La idea es crear un conjunto de soluciones nuevas a partir de las actuales. Esto se puede hacer identificando y combinano los atributos de las soluciones actuales. Los operadores de recombinación ciegos, como los usados tradicionalmente por los algoritmos genéticos, no incorporan ningún conocimiento del dominio al momento de generar nuevos individuos, por ejemplo, cruza uniforme. El argumento principal para no introducir conocimiento es para no sesgar la búsqueda, y evitar convergencias prematuras a soluciones subóptimas, sin embargo, esto último ha sido cuestionado últimamente. Los operadores que introducen conocimiento se llaman heuristico híbrido. El conocimiento se puede incorporar en: La selección de los atributos de los padres que van a ser transmitidos a los hijos.La selección de los atributos que no son de los padres que van a ser transmitidos a los hijos.
La idea principal de la selección por torneo consiste en realizar la selección en base a comparaciones directas entre individuos. Existen dos versiones de selección mediante torneo: Determinística Probabilística En la versión determinística se selecciona al azar un número p de individuos. De entre los individuos seleccionados se selecciona el más apto para pasarlo a la siguiente generación.La versión probabilística únicamente se diferencia en el paso de selección del ganador del torneo. En vez de escoger siempre el mejor se genera un número aleatorio del intervalo [0..1], si es mayor que un parámetro p (fijado para todo el proceso evolutivo) se escoge el individuo más alto y en caso contrario el menos apto.
La Selección por ruleta Propuesto por DeJong, es posiblemente el método más utilizado desde los orígenes de los Algoritmos Genéticos [Blickle and Thiele, 1995]. A cada uno de los individuos de la población se le asigna una parte proporcional a su ajuste de una ruleta, de tal forma que la suma de todos los porcentajes sea la unidad. Los mejores individuos recibirán una porción de la ruleta mayor que la recibida por los peores. Generalmente la población está ordenada en base al ajuste por lo que las porciones más grandes se encuentran al inicio de la ruleta. Para seleccionar un individuo basta con generar un número aleatorio del intervalo [0..1] y devolver el individuo situado en esa posición de la ruleta. Esta posición se suele obtener recorriendo los individuos de la población y acumulando sus proporciones de ruleta hasta que la suma exceda el valor obtenido. 
Cruce: tiene una alta probabilidad de ser utilizado y es considerado como el más importante dentro de los AG. Permite la generación de nuevos individuos tomandos caracter´ısticas de individuos padres. Consiste en seleccionar dos individuos después del proceso de selcción, determinar una posición de cruce aleatoria e intercambiar las cadenas entre la posición inicial y el punto de cruce y el punto de cruce y la posición final. Existen diferentes tipos de cruza. (i) Cruza simple: un solo punto de cruza (una máscara de 1’s seguida de 0’s), (ii) cruza de dos puntos y (iii) cruza uniforme (generando una máscara con 1’s y 0’s siguiendo una distribución de Bernoulli). Existe evidencia empírica que sugiere que cruza en un punto no es la mejor opción.
Mutación: tiene baja probabilidad de ser utilizado y permite introducir nueva información no presente en la población. Opera sobre un solo individuo, determina una posición y la invierte con cierta probabilidad. Permite salir de máximos locales. En lugar de generar un número aleatorio para cada gen y ver si se muta o no, se puede generar un número que nos de el número de mutaciones a realizar (M) y simplemente generar M números  aleatorios entre 1 y el tamaño del gen. Lo único que falta por determinar es que valor usar cuando existan más de dos posibles valores
En el concepto de algoritmos genéticos, la función de aptitud actúa como brújula, guiando el proceso de optimización. Esta función evalúa la calidad de las posibles soluciones y asigna puntuaciones que dirigen el algoritmo hacia la ruta óptima. A medida que el algoritmo evoluciona a través de múltiples generaciones, la función de aptitud influye en qué soluciones sobreviven, se reproducen y contribuyen a las siguientes iteraciones.
elitismo: Un algoritmo genético, desde el punto de vista de la optimización, es un método poblacional de búsqueda dirigida basada en probabilidad. Bajo una condición bastante débil, que el algoritmo mantenga elitismo, es decir, guarde siempre al mejor elemento de la población sin hacerle ningún cambio, se puede demostrar que el algoritmo converge en probabilidad al óptimo. En otras palabras, al aumentar el número de iteraciones, la probabilidad de tener el óptimo en la población tiende a uno. Un algoritmo genético emula el comportamiento de una población de individuos que representan soluciones y que evoluciona en base a los principios de la evolución natural: reproducción mediante operadores genéticos y selección de los mejores individuos, correspondiendo éstos a las mejores soluciones del problema a optimizar.
La Búsqueda Dispersa (denominada en inglés Scatter Search y abreviadamente SS) es un procedimiento metaheurístico basado en formulaciones propuestas en los años sesenta sobre estrategias para combinar soluciones o reglas de decisión y que se ha aplicado con éxito a la resolución de una gran variedad de problemas de optimización. Aunque puede ser clasificado como un método de los que denominamos “evolutivos” o “basados en poblaciones” presenta diferencias importantes con respecto a otros métodos tales como el uso de estrategias sistemáticas en lugar de aleatorias. SS opera sobre un pequeño conjunto de soluciones denominado conjunto de referencia. Básicamente se trata de que mediante la combinación de las soluciones que forman el conjunto de referencia se obtengan nuevas soluciones que mejoren a las que las originaron. En este trabajo se exponen los conceptos y principios básicos de SS así como las múltiples alternativas que ofrece para explotar sus ideas fundamentales. Finalmente se muestra una aplicación del método a la resolución de un conocido problema de localización.
En ciencias de la computación y en investigación operativa, el algoritmo de la colonia de hormigas, algoritmo hormiga u optimización por colonia de hormigas es una técnica probabilística para solucionar problemas computacionales que pueden reducirse a buscar los mejores caminos o rutas en grafos. Este algoritmo es un miembro de la familia de los algoritmos de colonia de hormigas, dentro de los métodos de inteligencia de enjambres. Inicialmente propuesto por Marco Dorigo en 1992 en su tesis de doctorado, el primer algoritmo surgió como método para buscar el camino óptimo en un grafo, basado en el comportamiento de las hormigas cuando estas están buscando un camino entre la colonia y una fuente de alimentos. La idea original se ha diversificado para resolver una amplia clase de problemas numéricos, y como resultado, han surgido gran cantidad de problemas nuevos, basándose en diversos aspectos del comportamiento de las hormigas.En nuestro mundo natural, las hormigas (inicialmente) vagan de manera aleatoria, al azar, y una vez encontrada comida regresan a su colonia dejando un rastro de feromonas. Si otras hormigas encuentran dicho rastro, es probable que estas no sigan caminando aleatoriamente, puede que estas sigan el rastro de feromonas, regresando y reforzándolo si estas encuentran comida finalmente. Sin embargo, al paso del tiempo el rastro de feromonas comienza a evaporarse, reduciéndose así su fuerza de atracción. Cuanto más tiempo le tome a una hormiga viajar por el camino y regresar de vuelta otra vez, más tiempo tienen las feromonas para evaporarse. Un camino corto, en comparación, es marchado más frecuentemente, y por lo tanto la densidad de feromonas se hace más grande en caminos cortos que en los largos. La evaporación de feromonas también tiene la ventaja de evitar convergencias a óptimos locales. Si no hubiese evaporación en absoluto, los caminos elegidos por la primera hormiga tenderían a ser excesivamente atractivos para las siguientes hormigas. En este caso, el espacio de búsqueda de soluciones sería limitado. Por tanto, cuando una hormiga encuentra un buen camino entre la colonia y la fuente de comida, hay más posibilidades de que otras hormigas sigan este camino y con una retroalimentación positiva se conduce finalmente a todas las hormigas a un solo camino. La idea del algoritmo colonia de hormigas es imitar este comportamiento con "hormigas simuladas" caminando a través de un grafo que representa el problema en cuestión.
Los EDAs son algoritmos heurísticos de optimización que basan su búsqueda –al igual que los algoritmos genéticos– en el caracter estocástico de la misma. Tambien al igual que los algoritmos genéticos los EDAs están basados en poblaciones que evolucionan. Sin embargo a diferencia de los algoritmos genéticos en los EDAs la evolución de las poblaciones no se lleva a cabo por medio de los operadores de cruce y mutación. En lugar de ello la nueva población de individuos se muestrea de una distribución de probabilidad, la cual es estimada de la base de datos conteniendo al conjunto de individuos seleccionados de entre los que constituyen la generación anterior. Mientras que en los algoritmos genéticos las interrelaciones entre las variables representando a los individuos se tienen en cuenta de manera implícita, en los EDAs dichas interrelaciones se expresan de manera explícita a través de la distribución de probabilidad asociada con los individuos seleccionados en cada generación. De hecho la estimación de dicha distribución de probabilidad conjunta asociada a los individuos seleccionados en cada generación es la principal dificultad de esta aproximación.
El algoritmo CAA es un algoritmo de inteligencia de enjambre propuesto por Karaboga en 2005, el cual es inspirado en el comportamiento de abejas en la búsqueda de miel. Desde su desarrollo, ha sido aplicado para solucionar diferentes clases de problemas. El algoritmo colonia de abeja artificial (CAA) es una técnica de optimización recientemente propuesta qué simula el comportamiento inteligente de las abejas en la búsqueda de la miel. Un conjunto de abejas se denomina enjambre cuando pueden realizar tareas a través de la cooperación social de manera exitosa. En el algoritmo CAA, hay tres tipos de abejas: abejas empleadas, abejas en espera, y abejas exploradoras. Las abejas empleadas buscan comida alrededor de la posición de las fuentes de alimentos guardadas en sus memorias; entretanto comparten la información de estas fuentes de alimentos con las abejas en espera. Las abejas en espera tienden a seleccionar buenas fuentes de alimentos de aquellas encontradas por las abejas empleadas. Las fuente de alimento que tiene calidad más alta tendrá una posibilidad más grande de ser seleccionada por las abejas en espera que la de calidad más baja. Las abejas exploradoras se trasladan hasta una nueva fuente determinando el abandono de dicha fuente por las empleadas. En el algoritmo CAA, la primera mitad del enjambre consta de abejas empleadas, y la segunda mitad constituye las abejas en espera. El número de abejas empleadas o de abejas en espera es igual al número de soluciones en el enjambre. El CAA genera una población inicial aleatoriamente distribuida de SN soluciones (fuentes de alimentos), donde SN denota el tamaño del enjambre. 
El algoritmo de búsqueda del cuco (CSA) es una técnica de optimización metaheurística inspirada en la naturaleza desarrollada por Xin-She Yang y Suash Deb en 2009. Su base conceptual se basa en el comportamiento de parasitismo de cría observado en ciertas especies de cucos, que ponen sus huevos en los nidos de otras aves hospedadoras en lugar de construir sus propios nidos. En el contexto de la optimización, cada huevo de un nido representa una solución, y un huevo de cuco significa una nueva solución potencialmente superior. El algoritmo genera iterativamente generaciones posteriores de nidos que contienen los mejores huevos, manteniendo así las soluciones de mayor calidad. Una característica clave de CSA es el uso de vuelos de Lévy —un mecanismo de recorrido aleatorio con distribuciones de longitud de paso de cola pesada— que permite una exploración eficiente del espacio de búsqueda y mejora la capacidad de búsqueda global del algoritmo. Esto ayuda a evitar óptimos locales y a mejorar la calidad de la solución. El algoritmo se destaca por su simplicidad y eficiencia y se ha aplicado en diversos problemas de optimización , particularmente en los dominios de ingeniería y informática 
El algoritmo GWO imita la jerarquía de liderazgo y el mecanismo de caza de los lobos grises en la naturaleza. Se emplean cuatro tipos de lobos grises: alfa, beta, delta y omega, para simular la jerarquía de liderazgo. Además, se implementan tres pasos principales: caza: búsqueda de presas, cercamiento y ataque, para la optimización.
El algoritmo de luciérnagas (algoritmo F) fue propuesto en la Universidad de Cambridge (Reino Unido) en 2007 por X-Sh. Yang, e inmediatamente atrajo la atención de los investigadores de optimización. El algoritmo de luciérnagas forma parte de una familia de algoritmos de inteligencia de enjambre que recientemente han mostrado resultados impresionantes en la resolución de problemas de optimización. El algoritmo de luciérnagas, en particular, se utiliza para resolver problemas de optimización continua y discreta. El algoritmo de luciérnagas tiene tres reglas basadas en las características del destello de las luciérnagas reales. Aquí las tenemos: Todas las luciérnagas se volverán más atrayentes y brillantes El grado de atracción de una luciérnaga es proporcional a su brillo, que disminuirá a medida que aumente la distancia respecto a otra luciérnaga, debido a que el aire absorbe la luz. Por consiguiente, entre dos luciérnagas parpadeantes, la menos brillante se moverá hacia la más brillante. Si no hay una luciérnaga más brillante o más atrayente que una en particular, se moverá al azar. El brillo o intensidad de la luz de la luciérnaga estará determinado por el valor de la función objetivo del problema. Inicialmente, al comienzo del algoritmo, todas las luciérnagas se dispersan al azar por todo el espacio de búsqueda. Luego, el algoritmo determina las particiones óptimas considerando dos fases:  El cambio en la intensidad de la luz: el brillo de la luciérnaga en su posición actual se refleja en el valor de su estado físico, el movimiento hacia una luciérnaga atrayente. La luciérnaga cambia de posición al observar la intensidad de la luz de las luciérnagas próximas.
El algoritmo de búsqueda de armonía (HSA) es un algoritmo metaheurístico bien conocido, desarrollado por Geem et al. [ 5 ] en 2001. HSA está inspirado en el proceso de improvisación de los músicos. Un músico busca las notas perfectas para desarrollar una armonía perfecta. Con base en este concepto, HSA fue desarrollado para buscar una buena solución para un problema de optimización. Los desarrolladores de HSA realizaron una amplia experimentación para validar su rendimiento. Las principales características de HSA son las siguientes: (1) no se requiere la configuración inicial de variables de decisión, (2) no se requiere información derivada y (3) se necesitan pocos parámetros de control para el ajuste fino [ 6 ]. Debido a estas características, HSA se prefiere sobre las otras técnicas metaheurísticas existentes. HSA se utiliza para resolver la amplia variedad de problemas, como problemas de agrupamiento, problemas socioeconómicos, problemas de ingeniería energética, problemas de visión artificial, problemas de programación y problemas de optimización global. HSA se considera un algoritmo eficiente con una implementación simple. Algunos investigadores también investigaron la aplicabilidad de HSA en diferentes dominios
El proceso KDD es un proceso utilizado para llevar a cabo la extracción automatizada de conocimiento partiendo de grandes volúmenes de datos, el cual es de naturaleza iterativa, por lo tanto, es aplicable tantas veces como sea necesario hasta obtener la información necesaria. Normalmente el proceso KDD tiene como motivación la detección de información que permita resolver los problemas o necesidades que surgen en las empresas y es a menudo solicitado por directivos. El conocimiento que se pretende extraer con el proceso KDD debe ser no trivial, implícito, previamente desconocido y potencialmente útil.
Recopilación de datos de diferentes fuentes integrándose en un único reposito de llamado almacén de datos, aparte de la información interna de la organización, se puede recopilar e integrar información de fuentes externas. Los datos deben guardarse de forma segura y confiable. Los almacenes pueden ser físicos o lógicos.
La limpieza de datos, también llamada depuración de datos, es el proceso de identificar y corregir errores e incongruencias en conjuntos de datos sin procesar para mejorar la calidad de los datos, con el objetivo de garantizar que los datos sean precisos, completos, coherentes y utilizables para análisis o toma de decisiones. Los procesos de limpieza de datos funcionan para abordar problemas comunes de calidad de datos, como duplicados, valores faltantes, incongruencias, errores de sintaxis, datos irrelevantes y errores estructurales.
La integración de datos se refiere al proceso de combinar y armonizar datos de múltiples fuentes en un formato unificado y coherente que pueda emplear para diversos fines analíticos, operativos y de toma de decisiones.
Extraer, cargar, transformar (ELT) implica extraer datos de su fuente, cargarlos en una base de datos o almacén de datos y luego transformarlos en un formato que se adapte a las necesidades del negocio. Esto podría implicar limpiar, agregar o resumir los datos. Los pipelines de datos ELT se emplean comúnmente en proyectos de big data y procesamiento en tiempo real donde la velocidad y la escalabilidad son críticas.
La normalización de datos es una técnica utilizada en minería de datos para transformar los valores de un conjunto de datos a una escala común. Esto es importante porque muchos algoritmos de aprendizaje automático son sensibles a la escala de las características de entrada y pueden producir mejores resultados cuando los datos están normalizados. La normalización se utiliza para escalar los datos de un atributo de modo que se encuentren en un rango menor, como de -1,0 a 1,0 o de 0,0 a 1,0. Generalmente es útil para algoritmos de clasificación.
El preprocesamiento de datos es un aspecto clave de la preparación de datos. Se refiere a cualquier tratamiento aplicado a los datos brutos para prepararlos para posteriores tareas de análisis o procesamiento. Tradicionalmente, el preprocesamiento de datos ha sido un paso previo esencial en el análisis de datos. Sin embargo, más recientemente, estas técnicas se han adaptado para entrenar modelos de aprendizaje automático y de IA y hacer inferencias a partir de ellos. 
La reducción de datos simplifica el conjunto de datos reduciendo el número de características o registros, pero conservando la información esencial. Esto ayuda a acelerar el análisis y el entrenamiento del modelo sin sacrificar la precisión.
Los patrones frecuentes son conjuntos de elementos, subsecuencias o subestructuras que aparecen en un conjunto de datos con una frecuencia no inferior a un umbral especificado por el usuario. Por ejemplo, un conjunto de artículos, como leche y pan, que aparecen juntos con frecuencia en un conjunto de datos de transacciones, es un conjunto de elementos frecuente. Una subsecuencia, como comprar primero una computadora, luego una cámara digital y luego una tarjeta de memoria, si aparece con frecuencia en una base de datos de historial de compras, es un patrón secuencial (frecuente). Una subestructura puede referirse a diferentes formas estructurales, como subgrafos, subárboles o subredes, que pueden combinarse con conjuntos de elementos o subsecuencias. Si una subestructura aparece con frecuencia en una base de datos de grafos, se denomina patrón estructural (frecuente). Encontrar patrones frecuentes es esencial para extraer asociaciones, correlaciones y muchas otras relaciones interesantes entre los datos. Además, facilita la indexación, clasificación, agrupamiento y otras tareas de minería de datos. La minería de patrones frecuentes es una tarea importante de minería de datos y un tema central en la investigación sobre minería de datos. Se ha dedicado abundante literatura a esta investigación y se han logrado enormes avances, desde algoritmos eficientes y escalables para la minería frecuente de conjuntos de elementos en bases de datos transaccionales hasta numerosas fronteras de investigación, como la minería de patrones secuencial
La minería de datos se refiere a la extracción o minería de conocimiento de grandes cantidades de datos. En otras palabras, la minería de datos es la ciencia, el arte y la tecnología de descubrir grandes y complejos conjuntos de datos para descubrir patrones útiles. Tanto teóricos como profesionales buscan continuamente técnicas mejoradas para que el proceso sea más eficiente, rentable y preciso
Pronóstico de Series Temporales: El pronóstico es un método para realizar predicciones basadas en datos pasados y presentes para predecir el futuro. El análisis de tendencias es un método para pronosticar series temporales. Es una función que genera patrones históricos en series temporales que se utilizan en predicciones a corto y largo plazo. Podemos obtener diversos patrones en series temporales, como movimientos cíclicos, de tendencia y estacionales, según se observen con respecto al tiempo o la estación. ARIMA, SARIMA y el modelado de series temporales de memoria larga son algunos de los métodos más populares para este tipo de análisis.
La gobernanza de datos ayuda a garantizar la integridad y la seguridad de los datos mediante la definición e implementación de políticas, estándares y procedimientos para la recopilación, propiedad, almacenamiento, procesamiento y uso de datos. El objetivo de la gobernanza de datos es mantener datos seguros y de alta calidad a los que se pueda acceder fácilmente para las iniciativas de descubrimiento de datos y negocio inteligente. Actuando más bien como un centro de control de tráfico aéreo, la función de gobernanza de datos ayuda a garantizar que los datos verificados fluyan a través de canalizaciones seguras hacia endpoint y usuarios confiables.
La gestión de metadatos se refiere a la organización, optimización y uso de metadatos para mejorar la accesibilidad y la calidad de los datos de una organización. Definidos simplemente como "datos sobre datos", los metadatos incluyen información como el autor, la fecha de creación, el tamaño del archivo, las palabras clave y los elementos estructurales.
Las herramientas de linaje de metadatos rastrean el recorrido completo de los datos y admiten una amplia gama de casos de uso. A través del análisis de impacto, por ejemplo, las organizaciones pueden identificar cómo cualquier cambio en los datos afecta los procesos posteriores. Las herramientas de linaje también mejoran el cumplimiento normativo al garantizar la transparencia en los flujos y transformaciones de datos
Calidad de los datos: Las organizaciones pueden promover metadatos de alta calidad a través de prácticas eficaces de gestión de metadatos. Las herramientas de enriquecimiento automatizado, por ejemplo, pueden agregar contexto empresarial, clasificaciones y estadísticas de resumen. Las métricas clave, como la integridad, la precisión, la coherencia y la actualidad, ayudan a las organizaciones a medir y mejorar la confiabilidad de los metadatos. Estos insights, combinados con una curaduría eficaz de los metadatos, reducen los esfuerzos de catalogación manual y mejoran la usabilidad de los datos.
Los datos agregados son datos de alto nivel que se obtienen mediante la combinación de datos individuales. Por ejemplo, la producción de una industria es un agregado de la producción individual de las empresas dentro de esa industria. [ 1 ] Los datos agregados se aplican en estadística, almacenes de datos y economía.
Los modelos estadísticos utilizan ecuaciones matemáticas para codificar información extraída de los datos. En algunos casos, las técnicas de modelado estadístico pueden proporcionar modelos adecuados de forma rápida. Incluso en el caso de problemas en los que las técnicas más flexibles de aprendizaje de las máquinas (como redes neuronales) pueden ofrecer a la postre mejores resultados, es posible usar algunos modelos estadísticos como modelos predictivos de línea base para juzgar el rendimiento de técnicas más avanzadas.
Las reglas de asociación es una técnica de inteligencia artificial ampliamente utilizada en Data Mining. La realización de base de datos se ha vuelto una acción fundamental para las empresas, pero a consecuencia de la generación masiva de estos, nos encontramos frente a un problema, la infoxicación, disponemos de tanta información, que a veces es imposible organizarla con efectividad. Por ello, la clave está en descubrir patrones o algoritmos para sacarle el máximo partido, y aquí es donde entra en juego el Data Mining o minería de datos.
El suavizado de datos hace referencia a las técnicas para eliminar ruido o comportamientos no deseados en los datos, mientras que la detección de valores atípicos identifica puntos de datos que son significativamente diferentes del resto de los datos.
El enriquecimiento de datos es un proceso de administración de datos que mejora los conjuntos de datos existentes al agregar información relevante de fuentes internas o externas para hacerlos más robustos, precisos y valiosos. Se extiende más allá de la simple recopilación de datos para agregar contexto, atributos y significado que ayuden a las organizaciones a comprender mejor a los clientes, las operaciones y las condiciones del mercado. Los datos sin procesar a menudo carecen del contexto necesario para ofrecer insights reales del negocio. Por ejemplo, un registro de cliente puede contener solo un nombre y una dirección de correo electrónico, lo que ofrece poca comprensión del comportamiento o la intención. A través del enriquecimiento de datos, ese mismo registro se puede ampliar con datos de ubicación, industria, historial de compras y participación, creando un perfil de cliente más detallado y accionable que conduce a una analítica más inteligente y experiencias personalizadas. Cuando se combina con la automatización de la analítica, el enriquecimiento de datos ayuda a las organizaciones a transformar datos fragmentados en insights accionables de manera más rápida y con mayor precisión. Este foco en la toma de decisiones basadas en datos puede generar un impacto medible. Según Experian, el 88 % de las organizaciones afirman que estar impulsadas por datos les ayuda a mantenerse al día con las necesidades de los clientes y las tendencias del mercado, mientras que Forrester descubrió que las empresas que están avanzadas en sus esfuerzos de insights tienden a crecer al menos un 20 % más anualmente que aquellas que recién están comenzando.
